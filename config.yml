# ClawLayer Router Configuration

# LLM Providers Configuration
providers:
  # Local provider for embeddings
  local:
    url: http://localhost:11434
    type: ollama
    models:
      embed: nomic-embed-text
  
  # Remote provider for text generation
  remote:
    url: http://192.168.1.100:11434/v1/chat/completions
    type: openai  # openai-compatible API
    models:
      text: llama3.2
      vision: llava:latest

# Default provider assignments
defaults:
  embedding_provider: local
  text_provider: remote
  vision_provider: remote

# Server Configuration
server:
  port: 11435

# Router Configuration
routers:
  # Fast routers (regex/logic) - checked first
  fast:
    priority:
      - echo
      - command
    
    echo:
      enabled: true
    
    command:
      enabled: true
      prefix: "run:"
  
  # Semantic routers (embedding-based) - checked after fast routers
  semantic:
    priority:
      - greeting
      - summarize
    
    greeting:
      enabled: true
      provider: local  # Use local provider for embeddings
    
    summarize:
      enabled: true
      provider: local  # Use local provider for embeddings
