# ClawLayer Router Configuration

# LLM Providers Configuration
providers:
  # Local provider for embeddings
  local:
    url: http://localhost:11434
    type: ollama
    models:
      embed: nomic-embed-text
    capabilities:
      max_context: 8192
      tool_use: false
      agentic: false
  
  # Remote provider for text generation
  remote:
    url: http://192.168.1.100:11434/v1/chat/completions
    type: openai
    models:
      text: llama3.2
      vision: llava:latest
    capabilities:
      max_context: 131072
      tool_use: true
      agentic: false

# Default provider assignments
defaults:
  embedding_provider: local
  text_provider: remote
  vision_provider: remote

# Server Configuration
server:
  port: 11435

# Router Configuration
routers:
  # Fast routers (regex/logic) - checked first
  fast:
    priority:
      - echo
      - command
    
    echo:
      enabled: true
    
    command:
      enabled: true
      prefix: "run:"
  
  # Semantic routers (embedding) - checked after fast
  semantic:
    priority:
      - greeting
      - summarize
    
    greeting:
      enabled: true
      # Multi-stage cascade configuration
      stages:
        - provider: local
          model: nomic-embed-text
          threshold: 0.75  # High confidence required for stage 1
        - provider: remote  # Cascade to stage 2 if confidence < 0.75
          model: nomic-embed-text
          threshold: 0.6   # Lower threshold for stage 2
      utterances:
        - "hello"
        - "hi"
        - "hey"
        - "good morning"
        - "good afternoon"
        - "greetings"
    
    summarize:
      enabled: true
      # Multi-stage cascade configuration
      stages:
        - provider: local
          model: nomic-embed-text
          threshold: 0.7
        - provider: remote
          model: nomic-embed-text
          threshold: 0.5
      utterances:
        - "summarize the conversation"
        - "create a summary"
        - "checkpoint summary"
        - "structured context checkpoint"
        - "summarize"
