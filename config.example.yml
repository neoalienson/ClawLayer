# ClawLayer Router Configuration Example

# LLM Providers Configuration
providers:
  # Local provider for embeddings
  local:
    url: http://localhost:11434
    type: ollama
    provider_type: embedding
    models:
      embed: nomic-embed-text
  
  # Remote provider for text generation
  remote:
    url: http://192.168.1.100:11434/v1/chat/completions
    type: openai  # openai-compatible API
    provider_type: llm
    models:
      text: llama3.2
      vision: llava:latest
  
  # OpenAI provider example
  openai:
    url: https://api.openai.com/v1/chat/completions
    type: openai
    provider_type: llm
    models:
      text: gpt-4
      vision: gpt-4-vision-preview
      embed: text-embedding-3-small

# Default provider assignments
defaults:
  embedding_provider: local    # Use local for embeddings (fast)
  text_provider: remote        # Use remote for text generation
  vision_provider: remote      # Use remote for vision tasks

# Server Configuration
server:
  port: 11435

# Router Configuration
routers:
  # Fast routers (regex/logic-based) - checked first, no LLM/embedding needed
  fast:
    priority:
      - echo      # Tool result detection
      - command   # Command prefix detection
    
    echo:
      enabled: true
    
    command:
      enabled: true
      prefix: "run:"  # Command prefix to detect
  
  # Semantic routers (embedding-based) - checked after fast routers
  semantic:
    priority:
      - greeting   # Greeting detection via embeddings
      - summarize  # Summary request detection via embeddings
    
    greeting:
      enabled: true
      provider: local  # Use local provider for embeddings
    
    summarize:
      enabled: true
      provider: local  # Use local provider for embeddings
